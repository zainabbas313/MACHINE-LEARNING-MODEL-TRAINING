{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "yj3qINdEVBAW",
        "lv5mx0XHnsuQ",
        "_TXuHHeuorb-",
        "ijhUTEOLpjDB",
        "LBeArz48poTB",
        "aD5PsGVFqhOP",
        "3AAy4WN9rUzY",
        "Swk40lfArnsr",
        "qYAb99wmr7zS",
        "qVuGT1tQsba2",
        "tJt-7dqiaWHH",
        "VdKt_sWvawFG",
        "ybqA552Ka7TB",
        "v9U3JZQ1bA8Q",
        "yb_ICYKZbKnM",
        "KMG4Bi_neDcp",
        "QATxcijbbZAF",
        "JXik3eo7blCj",
        "iFsPVzVmbsCs",
        "iNfwf4yDdzm9",
        "f6naIuvpe2hx",
        "79NucROpe7N5",
        "3T5SiBDAfEW6",
        "kowSxBPRfRMe"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zainabbas313/MACHINE-LEARNING-MODEL-TRAINING/blob/main/SUPERVISED_THEORY.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TAXONOMY OF MACHINE LEARNING"
      ],
      "metadata": {
        "id": "5lQqmWLnTWDH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Machine learning algorithms can be categorized based on the type of learning they are designed to perform. This categorization is crucial for understanding the appropriate contexts and applications of different algorithms. Below is a brief explanation of the taxonomy of machine learning algorithms grouped by learning styles, as illustrated in Figure 4.1.\n",
        "\n",
        "### Supervised Learning Algorithms (a)\n",
        "\n",
        "In supervised learning, the algorithm is trained on a labeled dataset, which means that each training example is paired with an output label. The goal is to learn a mapping from inputs to outputs that can be used to predict the labels of new, unseen data.\n",
        "\n",
        "**Examples of Supervised Learning Algorithms:**\n",
        "- **Linear Regression:** Used for predicting a continuous value.\n",
        "- **Logistic Regression:** Used for binary classification tasks.\n",
        "- **Support Vector Machines (SVM):** Can be used for both classification and regression tasks.\n",
        "- **Decision Trees:** A versatile algorithm that can handle classification and regression.\n",
        "- **Random Forest:** An ensemble method that combines multiple decision trees to improve performance.\n",
        "- **Neural Networks:** Used for complex tasks such as image and speech recognition.\n",
        "\n",
        "### Unsupervised Learning Algorithms (b)\n",
        "\n",
        "Unsupervised learning algorithms work with data that does not have labeled responses. The goal is to infer the natural structure present within a set of data points.\n",
        "\n",
        "**Examples of Unsupervised Learning Algorithms:**\n",
        "- **K-Means Clustering:** Groups data points into a specified number of clusters based on their features.\n",
        "- **Hierarchical Clustering:** Builds a hierarchy of clusters for the data.\n",
        "- **Principal Component Analysis (PCA):** A dimensionality reduction technique used to reduce the number of features while preserving as much variability as possible.\n",
        "- **Autoencoders:** A type of neural network used for learning efficient codings of input data.\n",
        "\n",
        "### Semi-Supervised Learning Algorithms (c)\n",
        "\n",
        "Semi-supervised learning algorithms fall between supervised and unsupervised learning. They use a small amount of labeled data and a large amount of unlabeled data for training. This approach can significantly improve learning accuracy when obtaining labeled data is expensive or time-consuming.\n",
        "\n",
        "**Examples of Semi-Supervised Learning Algorithms:**\n",
        "- **Self-Training:** Starts with a supervised learning algorithm, labels the unlabeled data, and iteratively retrains the model.\n",
        "- **Co-Training:** Uses multiple classifiers trained on different views of the data and exchanges the most confident predictions on unlabeled data.\n",
        "- **Graph-Based Methods:** Utilize graph structures to propagate labels from labeled to unlabeled data points based on their relationships.\n",
        "\n",
        "### Summary\n",
        "\n",
        "Figure 4.1 categorizes machine learning algorithms into three primary learning styles:\n",
        "\n",
        "- **Supervised Learning (a):** Uses labeled data to predict outcomes and includes algorithms like linear regression, decision trees, and neural networks.\n",
        "- **Unsupervised Learning (b):** Finds hidden patterns or intrinsic structures in input data without labeled responses, including algorithms like K-means and PCA.\n",
        "- **Semi-Supervised Learning (c):** Combines a small amount of labeled data with a large amount of unlabeled data to improve learning performance, with techniques such as self-training and graph-based methods.\n",
        "\n",
        "This taxonomy helps in understanding the types of problems each class of algorithms can solve and choosing the appropriate algorithm based on the availability and nature of the data."
      ],
      "metadata": {
        "id": "T1ZFSHIXTSfW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Machine Learning Based on Similarity Testing"
      ],
      "metadata": {
        "id": "yj3qINdEVBAW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Machine learning algorithms can also be categorized based on how they handle similarity testing, which is fundamental to many predictive models. Below are explanations of three such approaches: regression algorithms, instance-based algorithms, and regularization algorithms.\n",
        "\n",
        "### Regression Algorithms\n",
        "\n",
        "**Description:**\n",
        "- Regression algorithms are a class of supervised learning models that predict a continuous outcome variable based on one or more predictor variables. The process involves creating a statistical model that minimizes the error between predicted values and actual values in the training data.\n",
        "\n",
        "**Key Points:**\n",
        "- **Supervised Approach:** Uses labeled training data.\n",
        "- **Error Minimization:** Iteratively refines the model by minimizing the prediction error.\n",
        "- **Predictive Modeling:** Commonly used for predicting numerical outcomes such as prices, temperatures, or sales figures.\n",
        "\n",
        "**Example Algorithms:**\n",
        "- **Linear Regression:** Models the relationship between the dependent variable and one or more independent variables by fitting a linear equation.\n",
        "- **Polynomial Regression:** Extends linear regression by fitting a polynomial equation to the data.\n",
        "- **Support Vector Regression (SVR):** Uses support vector machines for regression tasks, focusing on minimizing prediction error while balancing model complexity.\n",
        "\n",
        "### Instance-Based Algorithms\n",
        "\n",
        "**Description:**\n",
        "- Instance-based algorithms, also known as memory-based learning, store instances of the training data and make predictions by comparing new instances to the stored examples. These algorithms perform a similarity test to find the most similar instances and base their predictions on these similarities.\n",
        "\n",
        "**Key Points:**\n",
        "- **Critical Training Data:** Utilizes a database of reliable examples to make predictions.\n",
        "- **Similarity Testing:** Compares new instances with stored instances to find the best match.\n",
        "- **Memory-Based:** The model relies on the stored instances rather than an explicit function learned from the training data.\n",
        "\n",
        "**Example Algorithms:**\n",
        "- **K-Nearest Neighbors (KNN):** Finds the k most similar instances in the training data and predicts the output based on these neighbors.\n",
        "- **Locally Weighted Learning:** Uses a local subset of the training data to make predictions, weighted by their distance to the query point.\n",
        "\n",
        "### Regularization Algorithms\n",
        "\n",
        "**Description:**\n",
        "- Regularization algorithms extend regression methods by adding a penalty for model complexity. The goal is to prevent overfitting, which occurs when a model learns the noise in the training data rather than the underlying pattern. Regularization encourages simpler models that generalize better to new data.\n",
        "\n",
        "**Key Points:**\n",
        "- **Extension of Regression:** Builds on regression models by incorporating penalties for complexity.\n",
        "- **Model Simplicity:** Prefers simpler models to avoid overfitting and improve generalization.\n",
        "- **Error Regularization:** Balances the model fit with the complexity penalty.\n",
        "\n",
        "**Example Algorithms:**\n",
        "- **Ridge Regression (L2 Regularization):** Adds a penalty equal to the square of the magnitude of the coefficients, which helps to keep the coefficients small.\n",
        "- **Lasso Regression (L1 Regularization):** Adds a penalty equal to the absolute value of the magnitude of the coefficients, which can lead to some coefficients being exactly zero, thus performing feature selection.\n",
        "- **Elastic Net:** Combines L1 and L2 regularization to balance the benefits of both methods.\n",
        "\n",
        "### Summary\n",
        "\n",
        "The three approaches to machine learning based on similarity testing are:\n",
        "\n",
        "1. **Regression Algorithms:** Focus on predicting continuous values by minimizing the error between predicted and actual values, typically used in various statistical modeling tasks.\n",
        "2. **Instance-Based Algorithms:** Rely on storing and comparing new data to existing instances in the training dataset, useful for classification and regression tasks where local similarity is important.\n",
        "3. **Regularization Algorithms:** Extend regression by adding penalties for model complexity to improve generalization and prevent overfitting, essential for creating robust predictive models."
      ],
      "metadata": {
        "id": "hA_pf99gU4lN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### COMPARSION OF REGRESSION, INSTANCE-BASED AND REGULARIZATION ALGORITHMS"
      ],
      "metadata": {
        "id": "lDUyzjkoVqqL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "| **Algorithm Type**      | **When to Use**                                                                                         | **Why to Use**                                                                                          | **Pros**                                                                                         | **Cons**                                                                                        |\n",
        "|-------------------------|--------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------|\n",
        "| **Regression Algorithms**| - When you need to predict a continuous output (e.g., house prices, stock prices).                     | - They provide a clear and interpretable model of the relationship between inputs and outputs.           | - Simple to understand and implement.<br>- Effective for linear relationships.<br>- Efficient computation. | - Poor performance on non-linear data unless properly transformed.<br>- Sensitive to outliers. |\n",
        "| **Instance-Based Algorithms**| - When you have a small to moderate-sized dataset.<br>- When you need flexibility in decision boundaries. | - They are non-parametric and can model complex relationships by relying on the distance metric.         | - Simple and intuitive.<br>- No training phase (except hyperparameter tuning).<br>- Adaptable to new data. | - Computationally expensive at prediction time.<br>- Sensitive to irrelevant features and noise. |\n",
        "| **Regularization Algorithms**| - When dealing with high-dimensional data.<br>- When overfitting is a concern.                        | - They help in preventing overfitting by penalizing large coefficients, leading to simpler models.        | - Reduces overfitting.<br>- Can perform feature selection (Lasso).<br>- Works well with linear models.     | - May be computationally intensive for large datasets.<br>- Requires careful tuning of regularization parameters. |\n",
        "\n",
        "### Detailed Explanation\n",
        "\n",
        "#### **Regression Algorithms**\n",
        "- **When to Use:** Suitable for problems where the outcome is a continuous variable and the relationship between inputs and outputs is relatively linear or can be made linear through transformations.\n",
        "- **Why to Use:** They provide a straightforward method to understand and quantify the relationship between variables, making them useful for prediction and inference.\n",
        "- **Pros:**\n",
        "  - **Simplicity:** Easy to implement and interpret.\n",
        "  - **Efficiency:** Generally computationally efficient.\n",
        "  - **Applicability:** Effective for linear relationships.\n",
        "- **Cons:**\n",
        "  - **Linearity Assumption:** Struggle with non-linear relationships without modification.\n",
        "  - **Sensitivity:** Can be significantly affected by outliers.\n",
        "\n",
        "#### **Instance-Based Algorithms**\n",
        "- **When to Use:** Ideal for scenarios with a well-defined similarity measure and when the dataset is not too large. Also useful when the decision boundary is complex and not easily defined by a parametric model.\n",
        "- **Why to Use:** They do not assume any underlying distribution and can adapt to complex patterns in the data by relying on the similarity of instances.\n",
        "- **Pros:**\n",
        "  - **Flexibility:** Can model complex decision boundaries.\n",
        "  - **No Training Phase:** Predictions are made based on stored data.\n",
        "  - **Adaptability:** Easily updateable with new data.\n",
        "- **Cons:**\n",
        "  - **Computational Cost:** Prediction can be slow, especially with large datasets.\n",
        "  - **Feature Sensitivity:** Performance can degrade with irrelevant or noisy features.\n",
        "\n",
        "#### **Regularization Algorithms**\n",
        "- **When to Use:** Best for high-dimensional data and situations where overfitting is a significant concern, such as when the number of features is large compared to the number of observations.\n",
        "- **Why to Use:** By adding a penalty for complexity, they help maintain model simplicity and improve generalization to new data.\n",
        "- **Pros:**\n",
        "  - **Overfitting Reduction:** Helps in creating more generalizable models.\n",
        "  - **Feature Selection:** Lasso can shrink some coefficients to zero, effectively selecting features.\n",
        "  - **Performance:** Works well in high-dimensional settings.\n",
        "- **Cons:**\n",
        "  - **Computational Intensity:** Can be more computationally demanding due to the optimization of the penalty term.\n",
        "  - **Parameter Tuning:** Requires careful selection and tuning of regularization parameters."
      ],
      "metadata": {
        "id": "StfqiyYuVeMB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### COMPARSION OF DECISION TREE, BAYESIAN AND CLUSTERING"
      ],
      "metadata": {
        "id": "pSWm-KHrbCHz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "| **Algorithm Type**       | **Description**                                                                                                                                       | **When to Use**                                                                                                         | **Why to Use**                                                                                                               | **Pros**                                                                                                                                         | **Cons**                                                                                                                                          |\n",
        "|--------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------|\n",
        "| **Decision Tree Algorithm** | The model uses a tree structure where data's target values guide decisions through nodes, leading to a prediction at leaf nodes. Trained for classification and regression. | - When interpretability is important.<br>- Suitable for both classification and regression tasks.<br>- When the relationship between features and target is complex and non-linear. | - Provides clear, interpretable decisions.<br>- Handles both numerical and categorical data.<br>- Requires little data preprocessing. | - Easy to understand and visualize.<br>- Handles non-linear relationships.<br>- Requires minimal data preprocessing.<br>- Can handle both numerical and categorical data. | - Prone to overfitting without proper pruning.<br>- Can create biased trees if some classes dominate.<br>- Unstable: small changes in data can lead to different trees. |\n",
        "| **Bayesian Algorithm**      | Uses probabilistic models (Bayesian networks) represented by a directed acyclic graph (DAG) of statistically independent random variables. Utilizes prior and posterior probabilities. | - When prior knowledge about the problem can be incorporated.<br>- Pattern recognition, feature extraction, and regression applications. | - Effective in incorporating prior knowledge.<br>- Provides probabilistic interpretation of predictions.<br>- Suitable for small datasets. | - Can handle missing data well.<br>- Provides probabilistic predictions.<br>- Incorporates prior knowledge.<br>- Effective with small datasets.  | - Computationally intensive for large datasets.<br>- Requires strong assumptions about the data distribution.<br>- Sensitive to the choice of priors. |\n",
        "| **Clustering Algorithm**      | Groups similar data objects into clusters using methods like centroid-based and hierarchical clustering. An unsupervised learning approach based on similarity testing.                   | - When the goal is to find inherent groupings in data.<br>- Suitable for exploratory data analysis.<br>- Image segmentation, customer segmentation, etc. | - Identifies natural groupings in data.<br>- Useful for exploratory analysis and pattern recognition.<br>- Reduces dimensionality. | - Effective for identifying patterns and groupings.<br>- Useful for exploratory data analysis.<br>- No need for labeled data.                      | - Choosing the number of clusters can be arbitrary.<br>- Sensitive to initial conditions and scaling of data.<br>- May struggle with different sized and density clusters. |\n",
        "\n",
        "### Detailed Explanation\n",
        "\n",
        "#### **Decision Tree Algorithm**\n",
        "- **Description:**\n",
        "  - Decision trees split the data into subsets based on the value of input features, creating a tree-like structure where each node represents a feature, each branch represents a decision rule, and each leaf represents an outcome.\n",
        "- **When to Use:**\n",
        "  - Use decision trees when interpretability is crucial, as they provide a clear, visual representation of decision-making. They are suitable for both classification and regression tasks, especially when the relationship between features and target variables is complex and non-linear.\n",
        "- **Why to Use:**\n",
        "  - They are intuitive and easy to understand, handle both numerical and categorical data, and require minimal preprocessing.\n",
        "- **Pros:**\n",
        "  - Easy to visualize and interpret.\n",
        "  - Handles both continuous and discrete data.\n",
        "  - Requires little data preparation.\n",
        "- **Cons:**\n",
        "  - Can easily overfit the training data if not properly pruned.\n",
        "  - Can be biased towards dominant classes.\n",
        "  - Small changes in the data can lead to completely different trees.\n",
        "\n",
        "#### **Bayesian Algorithm**\n",
        "- **Description:**\n",
        "  - Bayesian algorithms, such as Bayesian networks, represent a set of variables and their conditional dependencies using a directed acyclic graph (DAG). They use Bayes' theorem to update the probability estimates as more data becomes available.\n",
        "- **When to Use:**\n",
        "  - Suitable when prior knowledge can be incorporated into the model, in applications like pattern recognition, feature extraction, and regression, especially when the dataset is small.\n",
        "- **Why to Use:**\n",
        "  - They provide a probabilistic approach to prediction, can handle missing data effectively, and are suitable for small datasets.\n",
        "- **Pros:**\n",
        "  - Incorporates prior knowledge and updates with new data.\n",
        "  - Provides probabilistic predictions.\n",
        "  - Handles missing data well.\n",
        "- **Cons:**\n",
        "  - Computationally intensive for large datasets.\n",
        "  - Requires strong assumptions about data distributions.\n",
        "  - Sensitive to the choice of prior probabilities.\n",
        "\n",
        "#### **Clustering Algorithm**\n",
        "- **Description:**\n",
        "  - Clustering algorithms group similar data points into clusters. Common methods include centroid-based clustering (e.g., K-means) and hierarchical clustering. These methods are unsupervised and rely on similarity testing.\n",
        "- **When to Use:**\n",
        "  - Use clustering algorithms to discover natural groupings in the data, suitable for exploratory data analysis, image segmentation, customer segmentation, and more.\n",
        "- **Why to Use:**\n",
        "  - They help identify patterns and inherent structures in the data without needing labeled examples.\n",
        "- **Pros:**\n",
        "  - Effective for pattern and grouping identification.\n",
        "  - Useful for exploratory analysis.\n",
        "  - Does not require labeled data.\n",
        "- **Cons:**\n",
        "  - The number of clusters needs to be defined a priori.\n",
        "  - Sensitive to initial conditions and data scaling.\n",
        "  - May struggle with clusters of varying sizes and densities."
      ],
      "metadata": {
        "id": "NGFVzMJibNig"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### COMPARSION OF ASSOCIATION RULE LEARNING, ARTIFICIAL NEURAL NETWORK AND DEEP LEARNING ALGORITHMS"
      ],
      "metadata": {
        "id": "p-6lDZQMgXsR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "| **Algorithm Type**             | **Description**                                                                                                                                           | **When to Use**                                                                                                             | **Why to Use**                                                                                                                   | **Pros**                                                                                                                                            | **Cons**                                                                                                                                              |\n",
        "|--------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
        "| **Association Rule-Based Learning** | Generates inference rules to explain observed relationships between variables in large multidimensional datasets. An unsupervised learning method.              | - When you need to find relationships and associations in large datasets.<br>- Market basket analysis, recommendation systems. | - Identifies interesting associations and correlations.<br>- Useful for market basket analysis and cross-selling strategies.       | - Discovers hidden patterns and relationships.<br>- Useful for large datasets.<br>- Easy to understand and interpret.                                | - Can generate a large number of rules, which may be difficult to manage.<br>- Requires significant computational resources for large datasets.<br>- May produce irrelevant associations. |\n",
        "| **Artificial Neural Network (ANN) Algorithm** | Cognitive models inspired by biological neurons. Used to model complex relationships between inputs and outputs, typically for pattern matching and solving deep learning problems. | - When dealing with complex patterns and relationships.<br>- Image and speech recognition, natural language processing.     | - Effective in modeling non-linear relationships.<br>- Capable of learning and adapting to data.<br>- Suitable for a wide range of applications.      | - Can model complex, non-linear relationships.<br>- Adaptable and can learn from data.<br>- Versatile across different domains.                      | - Requires a large amount of data for training.<br>- Computationally intensive.<br>- Can be a \"black box\" with less interpretability.                |\n",
        "| **Deep Learning Algorithm**    | Extends from ANNs by using deeper and more complex neural networks with multiple layers. Mimics human brain processes in response to light, sound, and visual signals. | - When dealing with large datasets and complex tasks.<br>- Applications include image and speech recognition, natural language processing, autonomous systems. | - Handles large-scale data and complex tasks effectively.<br>- Suitable for semi-supervised learning with minimal labeled data.<br>- Excels in feature extraction and pattern recognition. | - Excels in handling high-dimensional data and complex tasks.<br>- Automatically extracts features.<br>- Achieves state-of-the-art performance in many domains. | - Requires significant computational power and resources.<br>- Needs large datasets for effective training.<br>- Difficult to interpret and understand the learned models. |\n",
        "\n",
        "### Detailed Explanation\n",
        "\n",
        "#### **Association Rule-Based Learning**\n",
        "- **Description:**\n",
        "  - This method is used to find interesting relationships or associations between variables in large datasets. It generates inference rules, such as \"If a customer buys item A, they are likely to buy item B.\"\n",
        "- **When to Use:**\n",
        "  - Ideal for market basket analysis, recommendation systems, and any scenario where understanding the relationships between items is valuable.\n",
        "- **Why to Use:**\n",
        "  - Helps in discovering hidden patterns and relationships in data, which can be used for strategic decisions in marketing and sales.\n",
        "- **Pros:**\n",
        "  - Can uncover unexpected associations and patterns.\n",
        "  - Works well with large datasets.\n",
        "  - Easy to interpret and apply.\n",
        "- **Cons:**\n",
        "  - Can generate a large number of rules, making it difficult to manage.\n",
        "  - Computationally intensive for very large datasets.\n",
        "  - May produce irrelevant or trivial associations without proper constraints.\n",
        "\n",
        "#### **Artificial Neural Network (ANN) Algorithm**\n",
        "- **Description:**\n",
        "  - ANNs are inspired by the structure and function of biological neurons. They consist of interconnected neurons (nodes) that work together to model complex relationships between inputs and outputs.\n",
        "- **When to Use:**\n",
        "  - Suitable for tasks involving pattern recognition, such as image and speech recognition, and where the relationship between inputs and outputs is complex and non-linear.\n",
        "- **Why to Use:**\n",
        "  - ANNs are highly adaptable and capable of learning from data, making them suitable for a wide range of applications.\n",
        "- **Pros:**\n",
        "  - Capable of modeling complex, non-linear relationships.\n",
        "  - Adaptable and learn directly from data.\n",
        "  - Versatile and applicable to various domains.\n",
        "- **Cons:**\n",
        "  - Requires a large amount of data for training to perform well.\n",
        "  - Computationally intensive, requiring significant processing power.\n",
        "  - Often considered a \"black box,\" making it difficult to interpret the model's decisions.\n",
        "\n",
        "#### **Deep Learning Algorithm**\n",
        "- **Description:**\n",
        "  - Deep learning algorithms extend ANNs by adding more layers (depth) to the network, allowing them to model even more complex relationships and features. They are designed to mimic human brain processes in response to stimuli.\n",
        "- **When to Use:**\n",
        "  - Suitable for applications requiring high performance on large and complex datasets, such as image and speech recognition, natural language processing, and autonomous systems.\n",
        "- **Why to Use:**\n",
        "  - Deep learning algorithms excel in handling large-scale and high-dimensional data, automatically extracting features and achieving state-of-the-art performance in many domains.\n",
        "- **Pros:**\n",
        "  - Handles high-dimensional and complex data effectively.\n",
        "  - Automatically performs feature extraction.\n",
        "  - Achieves high performance in many challenging tasks.\n",
        "- **Cons:**\n",
        "  - Requires significant computational resources and power.\n",
        "  - Needs large amounts of data for effective training.\n",
        "  - Often difficult to interpret and understand the learned models (black box nature)."
      ],
      "metadata": {
        "id": "p-OBdpO0gnNO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### COMPARSION OF DIMENSIONAL REDUCTION, SUPPORT VECTOR MACHINE AND ENSEMBLE ALGORITHMS"
      ],
      "metadata": {
        "id": "y9xZYu6FjVSX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "| **Algorithm Type**          | **Description**                                                                                                                                     | **When to Use**                                                                                                          | **Why to Use**                                                                                                                    | **Pros**                                                                                                                                            | **Cons**                                                                                                                                              |\n",
        "|-----------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
        "| **Dimensional Reduction Algorithm** | These algorithms reduce the number of random variables under consideration by exploiting the inherent structure in the data, typically in an unsupervised manner. | - When dealing with high-dimensional data.<br>- To visualize multi-dimensional data.<br>- To reduce data size for efficiency. | - Summarizes data with fewer dimensions.<br>- Reduces computational cost.<br>- Helps in data visualization.                     | - Reduces data complexity.<br>- Helps in visualizing multi-dimensional data.<br>- Can improve model performance by removing noise.                    | - May lead to loss of information.<br>- The choice of dimensions to keep can be arbitrary.<br>- Can be computationally intensive for very large datasets. |\n",
        "| **Support Vector Machine (SVM) Algorithm** | These supervised learning algorithms are used for classification and regression tasks by finding a hyperplane that best separates different classes in the data. | - When you need a powerful classification method.<br>- Suitable for small to medium-sized datasets.<br>- High-dimensional spaces. | - Provides robust classification and regression.<br>- Effective in high-dimensional spaces.<br>- Works well with clear margin separation. | - Effective in high-dimensional spaces.<br>- Robust to overfitting in high-dimensional space.<br>- Can handle non-linear classification using kernels. | - Can be computationally intensive.<br>- Sensitive to the choice of kernel and hyperparameters.<br>- Not suitable for very large datasets.              |\n",
        "| **Ensemble Algorithm**      | These models combine multiple weaker models (learners) to create a stronger overall model. The combination is designed to improve prediction accuracy. | - When individual models perform poorly.<br>- When you need robust and accurate predictions.<br>- Competitions and real-world applications. | - Improves overall model accuracy.<br>- Reduces overfitting and variance.<br>- Can handle diverse types of weak learners.       | - Often provides superior performance.<br>- Reduces overfitting.<br>- Flexible in combining different models.<br>- Robust to errors in individual models. | - Can be complex to implement.<br>- Computationally intensive.<br>- Difficult to interpret individual model contributions.<br>- Requires careful tuning of parameters. |\n",
        "\n",
        "### Detailed Explanation\n",
        "\n",
        "#### **Dimensional Reduction Algorithm**\n",
        "- **Description:**\n",
        "  - These algorithms aim to reduce the number of random variables under consideration by summarizing or describing the data using fewer dimensions. Common techniques include Principal Component Analysis (PCA) and t-Distributed Stochastic Neighbor Embedding (t-SNE).\n",
        "- **When to Use:**\n",
        "  - Ideal for high-dimensional data where visualization, storage, and computational efficiency are concerns. Useful for preprocessing data to remove noise and redundancy.\n",
        "- **Why to Use:**\n",
        "  - Reducing dimensionality can lead to simpler models, lower computational costs, and easier data visualization.\n",
        "- **Pros:**\n",
        "  - Reduces the complexity of data.\n",
        "  - Helps in visualizing multi-dimensional data.\n",
        "  - Can improve model performance by removing noise.\n",
        "- **Cons:**\n",
        "  - Potential loss of important information.\n",
        "  - The choice of how many dimensions to keep can be somewhat arbitrary.\n",
        "  - Computationally intensive for very large datasets.\n",
        "\n",
        "#### **Support Vector Machine (SVM) Algorithm**\n",
        "- **Description:**\n",
        "  - SVMs are supervised learning algorithms used for classification and regression tasks. They work by finding the hyperplane that best separates different classes in the training data. SVMs can be extended to non-linear classification using kernel functions.\n",
        "- **When to Use:**\n",
        "  - Suitable for problems where the data has clear margin separation, especially in high-dimensional spaces. Effective for both small to medium-sized datasets.\n",
        "- **Why to Use:**\n",
        "  - SVMs are powerful for classification tasks, particularly when dealing with high-dimensional data and non-linear relationships through the use of kernels.\n",
        "- **Pros:**\n",
        "  - Effective in high-dimensional spaces.\n",
        "  - Robust to overfitting, especially in high-dimensional settings.\n",
        "  - Can handle non-linear classification using various kernels.\n",
        "- **Cons:**\n",
        "  - Computationally intensive, especially with large datasets.\n",
        "  - Sensitive to the choice of kernel and hyperparameters.\n",
        "  - May not perform well with overlapping classes.\n",
        "\n",
        "#### **Ensemble Algorithm**\n",
        "- **Description:**\n",
        "  - Ensemble algorithms combine multiple weaker models (often called weak learners) to create a stronger overall model. Common ensemble methods include Bagging (e.g., Random Forests), Boosting (e.g., AdaBoost, Gradient Boosting), and Stacking.\n",
        "- **When to Use:**\n",
        "  - When individual models are insufficient or perform poorly. Useful for achieving high accuracy and robustness in predictions, often used in competitions and real-world applications.\n",
        "- **Why to Use:**\n",
        "  - Ensemble methods improve the accuracy and robustness of predictions by combining the strengths of multiple models and reducing the likelihood of overfitting.\n",
        "- **Pros:**\n",
        "  - Often provides superior performance compared to individual models.\n",
        "  - Reduces overfitting and variance.\n",
        "  - Flexible in combining different models.\n",
        "  - Robust to errors in individual models.\n",
        "- **Cons:**\n",
        "  - Complex to implement and interpret.\n",
        "  - Computationally intensive.\n",
        "  - Difficult to determine the contribution of individual models.\n",
        "  - Requires careful tuning of parameters."
      ],
      "metadata": {
        "id": "sVBNzJK8juF2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SUPERVISED ML ALGORITHMS"
      ],
      "metadata": {
        "id": "lv5mx0XHnsuQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "| **ML Algorithm Class** | **Algorithm Names**                                                                                                 |\n",
        "|------------------------|---------------------------------------------------------------------------------------------------------------------|\n",
        "| **Regression**         | Linear, Polynomial, Logistic, Stepwise, OLSR (Ordinary Least Squares Regression), LOESS (Locally Estimated Scatterplot Smoothing), MARS (Multivariate Adaptive Regression Splines) |\n",
        "| **Classification**     | KNN (k-nearest Neighbor), Trees, Naïve Bayesian, SVM (Support Vector Machine), LVQ (Learning Vector Quantization), SOM (Self-Organizing Map), LWL (Locally Weighted Learning)      |\n",
        "| **Decision Trees**     | Decision Trees, Random Forests, CART (Classification and Regression Tree), ID3 (Iterative Dichotomiser 3), CHAID (Chi-squared Automatic Interaction Detection)                       |\n",
        "| **Bayesian Networks**  | Naïve Bayesian, Gaussian, Multinomial, AODE (Averaged One-Dependence Estimators), BBN (Bayesian Belief Network), BN (Bayesian Network)                                               |\n"
      ],
      "metadata": {
        "id": "HI0GGK83nxbg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## UNSUPERVISED ML ALGORITHMS"
      ],
      "metadata": {
        "id": "s_fvNkW9n3to"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "| **ML Algorithm Class**        | **Algorithm Names**                                                                                                                 |\n",
        "|-------------------------------|-------------------------------------------------------------------------------------------------------------------------------------|\n",
        "| **Association Analysis**      | A priori, Association Rules, Eclat, FP-Growth                                                                                        |\n",
        "| **Clustering**                | Clustering Analysis, k-means, Hierarchical Clustering, Expectation Maximization (EM), Density-based Clustering                        |\n",
        "| **Dimensionality Reduction**  | PCA (Principal Component Analysis), Discriminant Analysis, MDS (Multi-Dimensional Scaling)                                           |\n",
        "| **Artificial Neural Networks**| Perceptron, Back Propagation, RBFN (Radial Basis Function Network)                                                                   |\n"
      ],
      "metadata": {
        "id": "vduWzJx6oBis"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Comparison of various machine learning algorithms grouped by their respective categories, along with their advantages, disadvantages, and suitable datasets:\n",
        "\n",
        "| Category            | Algorithm                                                | When to Use                                                  | Pros                                                          | Cons                                                          | Suitable Dataset                                           |\n",
        "|---------------------|----------------------------------------------------------|--------------------------------------------------------------|---------------------------------------------------------------|---------------------------------------------------------------|------------------------------------------------------------|\n",
        "| Regression          | Linear Regression                                        | When predicting continuous outcomes with linear relationships | - Simple and interpretable                                    | - Assumes linear relationship between features and target   | Datasets with linear relationships between features and target variables |\n",
        "|                     | Polynomial Regression                                    | When the relationship between variables is non-linear        | - Captures non-linear relationships between variables        | - Susceptible to overfitting                                  | Datasets with non-linear relationships between features and target variables |\n",
        "|                     | Logistic Regression                                      | When predicting binary outcomes                              | - Provides probabilities                                      | - Assumes linearity in log-odds                               | Binary classification problems, such as spam detection      |\n",
        "|                     | Stepwise Regression                                      | When selecting relevant features for the model               | - Automatic feature selection                                 | - Prone to overfitting with large number of features          | Datasets with a large number of features                    |\n",
        "|                     | OLSR (Ordinary Least Squares Regression)                 | When fitting a simple linear model with least squares        | - Fast and efficient fitting                                   | - Sensitive to outliers and multicollinearity                | Datasets with normally distributed errors                   |\n",
        "|                     | LOESS (Locally Estimated Scatterplot Smoothing)          | When fitting a flexible non-parametric regression model     | - Captures complex non-linear relationships                   | - Computationally intensive and sensitive to tuning          | Datasets with non-linear relationships and potential outliers |\n",
        "|                     | MARS (Multivariate Adaptive Regression Splines)           | When modeling complex non-linear relationships               | - Captures interactions between variables                     | - Requires careful interpretation due to complexity          | Datasets with complex relationships and interactions        |\n",
        "| Classification      | KNN (k-nearest Neighbor)                                 | When classifying data based on similarity                   | - Simple and easy to understand                               | - Sensitive to noisy data and choice of k                     | Datasets with clear separation between classes               |\n",
        "|                     | Trees                                                    | When building interpretable decision rules                  | - Easy to interpret and visualize                              | - Prone to overfitting with deep trees                        | Datasets where decision boundaries are easily represented    |\n",
        "|                     | Naïve Bayesian                                           | When assuming independence between features                  | - Works well with small datasets                               | - Strong independence assumption                               | Text classification, spam filtering                         |\n",
        "|                     | SVM (Support Vector Machine)                             | When dealing with high-dimensional data or non-linear data  | - Effective in high-dimensional spaces                         | - Not suitable for large datasets with lots of noise          | Image recognition, text classification                      |\n",
        "|                     | LVQ (Learning Vector Quantization)                        | When clustering data based on similarity                    | - Robust to noise and outliers                                 | - Requires careful tuning                                      | Clustering large datasets with clear clusters                |\n",
        "|                     | SOM (Self-Organizing Map)                                | When visualizing high-dimensional data or clustering        | - Preserves topological properties of input space              | - Sensitivity to initial parameters                            | Visualization of high-dimensional data, clustering           |\n",
        "|                     | LWL (Locally Weighted Learning)                          | When learning complex non-linear relationships               | - Adapts locally to the data                                   | - Computational complexity and memory requirements             | Non-parametric regression, time series forecasting           |\n",
        "| Decision Trees      | Decision Trees                                           | When building a simple, interpretable decision tree          | - Easy to understand and interpret                             | - Prone to overfitting with deep trees                        | Datasets with categorical and numerical features            |\n",
        "|                     | Random Forests                                           | When reducing overfitting in decision trees                  | - Reduces variance and overfitting                              | - Loss of interpretability compared to single decision trees  | Datasets with large number of features and noisy data        |\n",
        "|                     | CART (Classification and Regression Tree)                | When building binary decision trees                          | - Can handle numerical and categorical data                    | - Prone to overfitting with deep trees                        | Datasets with categorical and numerical features            |\n",
        "|                     | ID3 (Iterative Dichotomiser 3)                           | When building decision trees based on entropy                | - Simple and easy to understand                                 | - Sensitive to noisy data and small changes in data           | Datasets with categorical features and clear decision rules |\n",
        "|                     | CHAID (Chi-squared Automatic Interaction Detection)       | When building decision trees based on statistical significance | - Handles categorical and numerical data                        | - Sensitive to small sample sizes and missing values          | Datasets with categorical and numerical features            |\n",
        "| Bayesian Networks  | Naïve Bayesian                                           | When assuming independence between features                  | - Simple and fast to build                                     | - Strong independence assumption                               | Text classification, spam filtering                         |\n",
        "|                     | Gaussian                                                 | When assuming Gaussian distribution for continuous features  | - Robust to noise                                              | - Assumes Gaussian distribution for features                  | Continuous data with Gaussian distributions                  |\n",
        "|                     | Multinomial                                              | When dealing with discrete features                         | - Handles discrete data                                        | - Sensitive to feature scale                                    | Text classification, document classification                |\n",
        "|                     | AODE (Averaged One-Dependence Estimators)                | When building a Bayesian network with averaged estimators   | - Captures dependencies between features                       | - Computationally intensive                                    | Datasets with dependencies between features                  |\n",
        "|                     | BBN (Bayesian Belief Network)                            | When modeling complex dependencies between features         | - Flexible modeling of dependencies                            | - Requires large amounts of data for accurate modeling         | Datasets with complex dependencies and interactions          |\n",
        "|                     | BN (Bayesian Network)                                    | When modeling probabilistic relationships between variables  | - Captures uncertainty and dependencies                         | - Sensitive to prior specification and choice of structure     | Datasets with probabilistic dependencies and uncertainty    |\n",
        "\n",
        "This table summarizes the various machine learning algorithms, their use cases, advantages, disadvantages, and suitable datasets. Depending on the specific problem, one can choose the most appropriate algorithm based on these considerations."
      ],
      "metadata": {
        "id": "PJSskw3Hq_Nt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SUPERVISED ML ALGORITHMS CODE"
      ],
      "metadata": {
        "id": "LNntXuE4omib"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## REGRESSION ALGORITHMS"
      ],
      "metadata": {
        "id": "_TXuHHeuorb-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LINEAR REGRESSION"
      ],
      "metadata": {
        "id": "ijhUTEOLpjDB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yb_vKvwwRIEY",
        "outputId": "b21c6313-72d6-4b2d-e33d-3c9e60627235"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Linear Regression MSE: 0.03711379440797686\n"
          ]
        }
      ],
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train model\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate model\n",
        "y_pred = model.predict(X_test)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(\"Linear Regression MSE:\", mse)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### POLYNOMIAL REGRESSION"
      ],
      "metadata": {
        "id": "LBeArz48poTB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "# Create polynomial features\n",
        "poly = PolynomialFeatures(degree=2)\n",
        "X_poly = poly.fit_transform(X)\n",
        "\n",
        "# Split dataset\n",
        "X_train_poly, X_test_poly, y_train_poly, y_test_poly = train_test_split(X_poly, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train model\n",
        "model_poly = LinearRegression()\n",
        "model_poly.fit(X_train_poly, y_train_poly)\n",
        "\n",
        "# Evaluate model\n",
        "y_pred_poly = model_poly.predict(X_test_poly)\n",
        "mse_poly = mean_squared_error(y_test_poly, y_pred_poly)\n",
        "print(\"Polynomial Regression MSE:\", mse_poly)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H9T2QmYlpnjR",
        "outputId": "2418a319-5f76-40ea-b464-3c2cbd54e420"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Polynomial Regression MSE: 0.05062519368414204\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LOGISTIC REGRESSION"
      ],
      "metadata": {
        "id": "aD5PsGVFqhOP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "# Load dataset\n",
        "iris = load_iris()\n",
        "X_logistic, y_logistic = iris.data, iris.target\n",
        "\n",
        "# Split dataset\n",
        "X_train_logistic, X_test_logistic, y_train_logistic, y_test_logistic = train_test_split(X_logistic, y_logistic, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train model\n",
        "model_logistic = LogisticRegression()\n",
        "model_logistic.fit(X_train_logistic, y_train_logistic)\n",
        "\n",
        "# Evaluate model\n",
        "accuracy_logistic = model_logistic.score(X_test_logistic, y_test_logistic)\n",
        "print(\"Logistic Regression Accuracy:\", accuracy_logistic)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zMH7wQWgqYbz",
        "outputId": "3a613685-c189-4ac6-cad7-0fc28a1a6e64"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression Accuracy: 1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ORDINARY LEAST SQUARE REGRESSION"
      ],
      "metadata": {
        "id": "3AAy4WN9rUzY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import statsmodels.api as sm\n",
        "\n",
        "# Add constant term to X\n",
        "X_ols = sm.add_constant(X)\n",
        "\n",
        "# Split dataset\n",
        "X_train_ols, X_test_ols, y_train_ols, y_test_ols = train_test_split(X_ols, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train model\n",
        "model_ols = sm.OLS(y_train_ols, X_train_ols)\n",
        "results = model_ols.fit()\n",
        "\n",
        "# Evaluate model\n",
        "y_pred_ols = results.predict(X_test_ols)\n",
        "mse_ols = mean_squared_error(y_test_ols, y_pred_ols)\n",
        "print(\"OLS Regression MSE:\", mse_ols)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "meP-Ez57qnJM",
        "outputId": "d1d0118f-2168-4885-a176-93f2b8c2ca15"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OLS Regression MSE: 0.03711379440797683\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### STEP-WISE REGRESSION"
      ],
      "metadata": {
        "id": "Swk40lfArnsr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import statsmodels.api as sm\n",
        "from sklearn.datasets import make_regression\n",
        "\n",
        "# Generate sample data\n",
        "np.random.seed(42)\n",
        "X, y = make_regression(n_samples=100, n_features=10, noise=0.1)\n",
        "\n",
        "# Convert to DataFrame for convenience\n",
        "X_df = pd.DataFrame(X, columns=[f'feature_{i}' for i in range(X.shape[1])])\n",
        "y_df = pd.Series(y, name='target')\n",
        "\n",
        "def forward_selection(X, y, significance_level=0.05):\n",
        "    initial_features = []\n",
        "    remaining_features = list(X.columns)\n",
        "    best_features = initial_features[:]\n",
        "\n",
        "    while remaining_features:\n",
        "        p_values = pd.Series(index=remaining_features)\n",
        "        for feature in remaining_features:\n",
        "            model = sm.OLS(y, sm.add_constant(X[best_features + [feature]])).fit()\n",
        "            p_values[feature] = model.pvalues[feature]\n",
        "\n",
        "        min_p_value = p_values.min()\n",
        "        if min_p_value < significance_level:\n",
        "            best_feature = p_values.idxmin()\n",
        "            remaining_features.remove(best_feature)\n",
        "            best_features.append(best_feature)\n",
        "        else:\n",
        "            break\n",
        "\n",
        "    return best_features\n",
        "\n",
        "# Perform forward selection\n",
        "selected_features = forward_selection(X_df, y_df)\n",
        "print(f'Selected features: {selected_features}')\n",
        "\n",
        "# Create the final model with selected features\n",
        "final_model = sm.OLS(y_df, sm.add_constant(X_df[selected_features])).fit()\n",
        "print(final_model.summary())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XU-vwH_sreGN",
        "outputId": "605b22f2-378a-4264-bae1-f429819a08ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected features: ['feature_6', 'feature_4', 'feature_5', 'feature_3', 'feature_9', 'feature_1', 'feature_0', 'feature_7', 'feature_2', 'feature_8']\n",
            "                            OLS Regression Results                            \n",
            "==============================================================================\n",
            "Dep. Variable:                 target   R-squared:                       1.000\n",
            "Model:                            OLS   Adj. R-squared:                  1.000\n",
            "Method:                 Least Squares   F-statistic:                 3.509e+07\n",
            "Date:                Thu, 13 Jun 2024   Prob (F-statistic):          6.22e-289\n",
            "Time:                        03:40:25   Log-Likelihood:                 92.202\n",
            "No. Observations:                 100   AIC:                            -162.4\n",
            "Df Residuals:                      89   BIC:                            -133.7\n",
            "Df Model:                          10                                         \n",
            "Covariance Type:            nonrobust                                         \n",
            "==============================================================================\n",
            "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
            "------------------------------------------------------------------------------\n",
            "const          0.0131      0.010      1.262      0.210      -0.008       0.034\n",
            "feature_6     87.0812      0.011   7997.104      0.000      87.060      87.103\n",
            "feature_4     93.6180      0.011   8607.755      0.000      93.596      93.640\n",
            "feature_5     70.6406      0.011   6550.914      0.000      70.619      70.662\n",
            "feature_3     63.6363      0.010   6162.838      0.000      63.616      63.657\n",
            "feature_9     70.9046      0.012   5939.021      0.000      70.881      70.928\n",
            "feature_1     54.1414      0.012   4686.567      0.000      54.118      54.164\n",
            "feature_0     16.7652      0.012   1424.562      0.000      16.742      16.789\n",
            "feature_7     10.4432      0.011    961.061      0.000      10.422      10.465\n",
            "feature_2      5.1843      0.011    458.599      0.000       5.162       5.207\n",
            "feature_8      3.1641      0.010    312.109      0.000       3.144       3.184\n",
            "==============================================================================\n",
            "Omnibus:                        5.421   Durbin-Watson:                   2.259\n",
            "Prob(Omnibus):                  0.066   Jarque-Bera (JB):                4.980\n",
            "Skew:                          -0.411   Prob(JB):                       0.0829\n",
            "Kurtosis:                       3.722   Cond. No.                         1.84\n",
            "==============================================================================\n",
            "\n",
            "Notes:\n",
            "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### MARS (MULTIVARIAVTE ADAPTIVE REGRESSION SPLINES)"
      ],
      "metadata": {
        "id": "qYAb99wmr7zS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyearth import Earth\n",
        "\n",
        "# Train model\n",
        "model_mars = Earth()\n",
        "model_mars.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate model\n",
        "y_pred_mars = model_mars.predict(X_test)\n",
        "mse_mars = mean_squared_error(y_test, y_pred_mars)\n",
        "print(\"MARS Regression MSE:\", mse_mars)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 401
        },
        "id": "yIIktgqqrqt8",
        "outputId": "87daff71-294e-4ba6-956a-8176b9c95c25"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "cannot import name 'Earth' from 'pyearth' (/usr/local/lib/python3.10/dist-packages/pyearth/__init__.py)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-c4bcb46b20d3>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpyearth\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mEarth\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Train model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmodel_mars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEarth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmodel_mars\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'Earth' from 'pyearth' (/usr/local/lib/python3.10/dist-packages/pyearth/__init__.py)",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LOESS (LOCALLY ESTIMATED SCATTEREDPLOT SMOOTHING)"
      ],
      "metadata": {
        "id": "qVuGT1tQsba2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import statsmodels.api as sm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Sample data\n",
        "X = np.random.rand(100) * 10\n",
        "y = 3.5 * X + np.random.randn(100) * 2\n",
        "\n",
        "# Create LOESS model\n",
        "lowess = sm.nonparametric.lowess(y, X, frac=0.2)\n",
        "\n",
        "# Extract the smoothed values\n",
        "X_lowess = lowess[:, 0]\n",
        "y_lowess = lowess[:, 1]\n",
        "\n",
        "# Plot the results\n",
        "plt.scatter(X, y, label='Data')\n",
        "plt.plot(X_lowess, y_lowess, color='red', label='LOESS Smoothed')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "id": "8-CbxSYcsLVM",
        "outputId": "687d4465-9ff4-42e3-df2b-f951a10957fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CLASSIFICATION"
      ],
      "metadata": {
        "id": "tJt-7dqiaWHH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### KNN (K-NEAREST NEIGHBOR)"
      ],
      "metadata": {
        "id": "VdKt_sWvawFG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train model\n",
        "model_knn = KNeighborsClassifier(n_neighbors=3)\n",
        "model_knn.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate model\n",
        "y_pred_knn = model_knn.predict(X_test)\n",
        "accuracy_knn = accuracy_score(y_test, y_pred_knn)\n",
        "print(\"KNN Accuracy:\", accuracy_knn)\n"
      ],
      "metadata": {
        "id": "Q7ailY2csPZi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ebd187a2-7fef-4aa2-e400-f88d6adaab2d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KNN Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DECISION TREE"
      ],
      "metadata": {
        "id": "ybqA552Ka7TB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# Train model\n",
        "model_tree = DecisionTreeClassifier()\n",
        "model_tree.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate model\n",
        "y_pred_tree = model_tree.predict(X_test)\n",
        "accuracy_tree = accuracy_score(y_test, y_pred_tree)\n",
        "print(\"Decision Tree Accuracy:\", accuracy_tree)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lCe0pSA-a6hq",
        "outputId": "c7df998c-69af-492f-c69e-4530785bc28f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### NAIVE BAYESIAN"
      ],
      "metadata": {
        "id": "v9U3JZQ1bA8Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "# Train model\n",
        "model_nb = GaussianNB()\n",
        "model_nb.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate model\n",
        "y_pred_nb = model_nb.predict(X_test)\n",
        "accuracy_nb = accuracy_score(y_test, y_pred_nb)\n",
        "print(\"Naïve Bayesian Accuracy:\", accuracy_nb)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RgRuRiMCa_t4",
        "outputId": "182b28b1-5746-49d3-dcc5-43db3fb146fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Naïve Bayesian Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### SUPPORT VECTOR MACHINE (SVM)"
      ],
      "metadata": {
        "id": "yb_ICYKZbKnM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####  Parameters of `SVC`:\n",
        "The `SVC` (Support Vector Classifier) in `scikit-learn` is a powerful and flexible machine learning model used for classification tasks. It offers a variety of parameters that allow you to fine-tune the model for optimal performance. Here are the parameters and their detailed descriptions:\n",
        "\n",
        "\n",
        "1. **`C`** (default=1.0):\n",
        "   - This parameter controls the regularization strength. Regularization is a technique to avoid overfitting by penalizing large coefficients.\n",
        "   - A smaller `C` value will create a larger margin but may increase misclassification, leading to a simpler model.\n",
        "   - A larger `C` value aims to classify all training examples correctly, potentially leading to a more complex model that may overfit.\n",
        "\n",
        "2. **`kernel`** (default='rbf'):\n",
        "   - This parameter specifies the kernel type to be used in the algorithm. It transforms the data into a higher-dimensional space.\n",
        "   - Common kernels:\n",
        "     - `'linear'`: No transformation, just a linear separation.\n",
        "     - `'poly'`: Polynomial kernel.\n",
        "     - `'rbf'`: Radial basis function (Gaussian) kernel.\n",
        "     - `'sigmoid'`: Sigmoid kernel.\n",
        "     - Custom kernel functions can also be defined.\n",
        "\n",
        "3. **`degree`** (default=3):\n",
        "   - This is the degree of the polynomial kernel function ('poly'). It is ignored by other kernels.\n",
        "   - Higher degrees make the decision boundary more complex.\n",
        "\n",
        "4. **`gamma`** (default='scale'):\n",
        "   - This parameter defines the influence of a single training example. It is only relevant for 'rbf', 'poly', and 'sigmoid' kernels.\n",
        "   - `'scale'`: `1 / (n_features * X.var())` (recommended).\n",
        "   - `'auto'`: `1 / n_features`.\n",
        "   - Can also be set to a specific float value.\n",
        "\n",
        "5. **`coef0`** (default=0.0):\n",
        "   - This parameter is used in 'poly' and 'sigmoid' kernels. It controls the influence of higher-degree polynomials in the decision function.\n",
        "\n",
        "6. **`shrinking`** (default=True):\n",
        "   - This boolean parameter specifies whether to use the shrinking heuristic, which can speed up the training process.\n",
        "\n",
        "7. **`probability`** (default=False):\n",
        "   - If set to True, it enables probability estimates, which can be more computationally expensive and require an additional cross-validation step.\n",
        "\n",
        "8. **`tol`** (default=1e-3):\n",
        "   - This parameter sets the tolerance for stopping criteria. It determines when to stop the training based on the precision of the optimization.\n",
        "\n",
        "9. **`cache_size`** (default=200):\n",
        "   - This parameter sets the size of the kernel cache (in MB), which can speed up computation for large datasets.\n",
        "\n",
        "10. **`class_weight`** (default=None):\n",
        "    - This parameter sets the weights associated with classes. It can help handle imbalanced datasets.\n",
        "    - `'balanced'` mode uses the values inversely proportional to class frequencies.\n",
        "\n",
        "11. **`verbose`** (default=False):\n",
        "    - This boolean parameter controls the verbosity of the training process.\n",
        "\n",
        "12. **`max_iter`** (default=-1):\n",
        "    - This parameter sets the maximum number of iterations for the optimization process.\n",
        "    - `-1` means no limit.\n",
        "\n",
        "13. **`decision_function_shape`** (default='ovr'):\n",
        "    - This parameter determines the shape of the decision function for multi-class classification.\n",
        "    - `'ovr'` (one-vs-rest): Default and recommended for most cases.\n",
        "    - `'ovo'` (one-vs-one): Used for comparing every pair of classes.\n",
        "\n",
        "14. **`break_ties`** (default=False):\n",
        "    - If set to True, this parameter ensures that the prediction is the class with the highest confidence score, which may be slower but can be more accurate for multi-class classification.\n",
        "\n",
        "15. **`random_state`** (default=None):\n",
        "    - This parameter sets the seed for the random number generator, useful for reproducibility.\n",
        "\n",
        "### Example Usage:\n",
        "Here's an example of how to use the `SVC` with different parameters:\n",
        "\n",
        "```python\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the model with custom parameters\n",
        "svc = SVC(\n",
        "    C=1.0,\n",
        "    kernel='linear',\n",
        "    degree=3,\n",
        "    gamma='scale',\n",
        "    coef0=0.0,\n",
        "    shrinking=True,\n",
        "    probability=False,\n",
        "    tol=1e-3,\n",
        "    cache_size=200,\n",
        "    class_weight=None,\n",
        "    verbose=False,\n",
        "    max_iter=-1,\n",
        "    decision_function_shape='ovr',\n",
        "    break_ties=False,\n",
        "    random_state=None\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "svc.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred = svc.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"SVC Accuracy:\", accuracy)\n",
        "```\n",
        "\n",
        "This example demonstrates setting various parameters for the `SVC` model. Adjusting these parameters can significantly impact the performance and computational efficiency of the model."
      ],
      "metadata": {
        "id": "KMG4Bi_neDcp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC\n",
        "\n",
        "# Train model\n",
        "model_svm = SVC(kernel='linear')\n",
        "model_svm.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate model\n",
        "y_pred_svm = model_svm.predict(X_test)\n",
        "accuracy_svm = accuracy_score(y_test, y_pred_svm)\n",
        "print(\"SVM Accuracy:\", accuracy_svm)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RsTn8GiJbGau",
        "outputId": "a087e46f-29c0-4d1d-826d-cf25643c8482"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SVM Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LVQ (LEARNING VECTOR QUANTIZATION)"
      ],
      "metadata": {
        "id": "QATxcijbbZAF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# LVQ is not directly available in scikit-learn. We can use `sklearn_lvq` library.\n",
        "# Install sklearn_lvq: pip install sklearn_lvq\n",
        "from sklearn_lvq import GlvqModel\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "model_lvq = GlvqModel()\n",
        "model_lvq.fit(X_train, y_train)\n",
        "y_pred_lvq = model_lvq.predict(X_test)\n",
        "accuracy_lvq = accuracy_score(y_test, y_pred_lvq)\n",
        "print(\"LVQ Accuracy:\", accuracy_lvq)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        },
        "id": "cf-6raNVbN5F",
        "outputId": "68a7c084-3aec-463f-ec00-90c67067e41e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "'x0' must only have one dimension.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-1151fa451dfc>\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mmodel_lvq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGlvqModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mmodel_lvq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0my_pred_lvq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_lvq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0maccuracy_lvq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred_lvq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn_lvq/lvq.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y)\u001b[0m\n\u001b[1;32m    102\u001b[0m             raise ValueError(\"fitting \" + type(\n\u001b[1;32m    103\u001b[0m                 self).__name__ + \" with only one class is not possible\")\n\u001b[0;32m--> 104\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn_lvq/glvq.py\u001b[0m in \u001b[0;36m_optimize\u001b[0;34m(self, x, y, random_state)\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_optimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m         \u001b[0mlabel_equals_prototype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_w_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m         res = minimize(\n\u001b[0m\u001b[1;32m    201\u001b[0m             fun=lambda vs: self._optfun(\n\u001b[1;32m    202\u001b[0m                 \u001b[0mvariables\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/scipy/optimize/_minimize.py\u001b[0m in \u001b[0;36mminimize\u001b[0;34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[0m\n\u001b[1;32m    531\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mx0\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 533\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"'x0' must only have one dimension.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    534\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    535\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mx0\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkind\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtypecodes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"AllInteger\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: 'x0' must only have one dimension."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### SOM (SELF-ORGANIZING MAP)"
      ],
      "metadata": {
        "id": "JXik3eo7blCj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install MiniSom: pip install minisom\n",
        "from minisom import MiniSom\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Standardize the dataset\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Split dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize and train SOM\n",
        "som = MiniSom(x=10, y=10, input_len=4, sigma=1.0, learning_rate=0.5)\n",
        "som.random_weights_init(X_train)\n",
        "som.train_random(X_train, 100)\n",
        "\n",
        "# Function to classify using SOM\n",
        "\n",
        "def classify_som(som, data, labels):\n",
        "    pred = []\n",
        "    for x in data:\n",
        "        w = som.winner(x)\n",
        "        if w in som.win_map:\n",
        "            win_positions = som.win_map[w]\n",
        "            class_labels = [labels[np.where(np.all(data == win_pos, axis=1))[0][0]] for win_pos in win_positions]\n",
        "            pred.append(np.argmax(np.bincount(class_labels)))\n",
        "        else:\n",
        "            pred.append(0)  # Default class if no winner\n",
        "    return np.array(pred)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred_som = classify_som(som, X_test, y_train)\n",
        "accuracy_som = accuracy_score(y_test, y_pred_som)\n",
        "print(\"SOM Accuracy:\", accuracy_som)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 348
        },
        "id": "UCytXeUJbfSR",
        "outputId": "172d7363-f33e-443b-dabc-68384d006a9f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "argument of type 'method' is not iterable",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-0f3fbd71dc5a>\u001b[0m in \u001b[0;36m<cell line: 40>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;31m# Predict and evaluate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m \u001b[0my_pred_som\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassify_som\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msom\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0maccuracy_som\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred_som\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"SOM Accuracy:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy_som\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-17-0f3fbd71dc5a>\u001b[0m in \u001b[0;36mclassify_som\u001b[0;34m(som, data, labels)\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwinner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwin_map\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m             \u001b[0mwin_positions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwin_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0mclass_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mwin_pos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mwin_pos\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwin_positions\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: argument of type 'method' is not iterable"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LWL (LOCALLY WEIGHTED LEARNING)"
      ],
      "metadata": {
        "id": "iFsPVzVmbsCs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Locally Weighted Learning isn't directly available in scikit-learn.\n",
        "# We'll use locally weighted linear regression for a simple example.\n",
        "import numpy as np\n",
        "\n",
        "def lwlr(testPoint, X, y, k=1.0):\n",
        "    m = X.shape[0]\n",
        "    weights = np.eye(m)\n",
        "    for j in range(m):\n",
        "        diffMat = testPoint - X[j]\n",
        "        weights[j, j] = np.exp(diffMat @ diffMat.T / (-2.0 * k**2))\n",
        "    xTx = X.T @ (weights @ X)\n",
        "    if np.linalg.det(xTx) == 0.0:\n",
        "        raise Exception(\"Singular matrix, cannot do inverse\")\n",
        "    ws = np.linalg.inv(xTx) @ (X.T @ (weights @ y))\n",
        "    return testPoint @ ws\n",
        "\n",
        "# Example usage\n",
        "test_point = X_test[0]\n",
        "pred = lwlr(test_point, X_train, y_train)\n",
        "print(\"LWL Prediction for test point:\", pred)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EoIWqvTmbpZ4",
        "outputId": "dea7aae7-2cbb-4d19-dd3b-e3daf834034a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LWL Prediction for test point: 1.0434816855452735\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DECISION TREE"
      ],
      "metadata": {
        "id": "iNfwf4yDdzm9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DECISION TREE CLASSIFIER"
      ],
      "metadata": {
        "id": "k8jdW8T6evw0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Parameters\n",
        "The `DecisionTreeClassifier` in scikit-learn is a versatile tool for classification tasks. Here, we discuss all the parameters of the `DecisionTreeClassifier`:\n",
        "\n",
        "\n",
        "1. **criterion**:\n",
        "   - Type: `{\"gini\", \"entropy\", \"log_loss\"}`, default=`\"gini\"`\n",
        "   - Description: This parameter determines the function used to measure the quality of a split. Supported criteria are:\n",
        "     - `\"gini\"` for the Gini impurity.\n",
        "     - `\"entropy\"` for the information gain.\n",
        "     - `\"log_loss\"` for log loss, which is equivalent to entropy.\n",
        "\n",
        "2. **splitter**:\n",
        "   - Type: `{\"best\", \"random\"}`, default=`\"best\"`\n",
        "   - Description: This parameter determines the strategy used to choose the split at each node. Supported strategies are:\n",
        "     - `\"best\"` to choose the best split.\n",
        "     - `\"random\"` to choose the best random split.\n",
        "\n",
        "3. **max_depth**:\n",
        "   - Type: `int`, default=`None`\n",
        "   - Description: The maximum depth of the tree. If `None`, then nodes are expanded until all leaves are pure or until all leaves contain less than `min_samples_split` samples.\n",
        "\n",
        "4. **min_samples_split**:\n",
        "   - Type: `int` or `float`, default=`2`\n",
        "   - Description: The minimum number of samples required to split an internal node:\n",
        "     - If `int`, then consider `min_samples_split` as the minimum number.\n",
        "     - If `float`, then `min_samples_split` is a fraction and `ceil(min_samples_split * n_samples)` are the minimum number of samples for each split.\n",
        "\n",
        "5. **min_samples_leaf**:\n",
        "   - Type: `int` or `float`, default=`1`\n",
        "   - Description: The minimum number of samples required to be at a leaf node. A split point at any depth will only be considered if it leaves at least `min_samples_leaf` training samples in each of the left and right branches. This may have the effect of smoothing the model, especially in regression.\n",
        "\n",
        "6. **min_weight_fraction_leaf**:\n",
        "   - Type: `float`, default=`0.0`\n",
        "   - Description: The minimum weighted fraction of the sum total of weights (of all the input samples) required to be at a leaf node. Samples have equal weight when sample_weight is not provided.\n",
        "\n",
        "7. **max_features**:\n",
        "   - Type: `int`, `float`, `{\"auto\", \"sqrt\", \"log2\"}`, default=`None`\n",
        "   - Description: The number of features to consider when looking for the best split:\n",
        "     - If `int`, then consider `max_features` features at each split.\n",
        "     - If `float`, then `max_features` is a fraction and `int(max_features * n_features)` features are considered at each split.\n",
        "     - If `\"auto\"`, then `max_features=sqrt(n_features)`.\n",
        "     - If `\"sqrt\"`, then `max_features=sqrt(n_features)` (same as `\"auto\"`).\n",
        "     - If `\"log2\"`, then `max_features=log2(n_features)`.\n",
        "     - If `None`, then `max_features=n_features`.\n",
        "\n",
        "8. **random_state**:\n",
        "   - Type: `int`, `RandomState` instance, or `None`, default=`None`\n",
        "   - Description: Controls the randomness of the estimator. The features are always randomly permuted at each split, even if `splitter` is set to `\"best\"`. When `random_state` is set, a fixed seed is used which helps in reproducing the results.\n",
        "\n",
        "9. **max_leaf_nodes**:\n",
        "   - Type: `int`, default=`None`\n",
        "   - Description: Grow a tree with `max_leaf_nodes` in best-first fashion. Best nodes are defined as relative reduction in impurity. If `None` then unlimited number of leaf nodes.\n",
        "\n",
        "10. **min_impurity_decrease**:\n",
        "    - Type: `float`, default=`0.0`\n",
        "    - Description: A node will be split if this split induces a decrease of the impurity greater than or equal to this value. The weighted impurity decrease equation is:\n",
        "      - `N_t / N * (impurity - N_t_R / N_t * right_impurity - N_t_L / N_t * left_impurity)`\n",
        "      - where `N` is the total number of samples, `N_t` is the number of samples at the current node, `N_t_L` is the number of samples in the left child, and `N_t_R` is the number of samples in the right child.\n",
        "\n",
        "11. **class_weight**:\n",
        "    - Type: `dict`, `list of dict`, `balanced` or `None`, default=`None`\n",
        "    - Description: Weights associated with classes in the form `{class_label: weight}`. If `None`, all classes are supposed to have weight one. The `balanced` mode uses the values of `y` to automatically adjust weights inversely proportional to class frequencies in the input data as `n_samples / (n_classes * np.bincount(y))`.\n",
        "\n",
        "12. **ccp_alpha**:\n",
        "    - Type: `non-negative float`, default=`0.0`\n",
        "    - Description: Complexity parameter used for Minimal Cost-Complexity Pruning. The subtree with the largest cost complexity that is smaller than `ccp_alpha` will be chosen. By default, no pruning is performed.\n",
        "\n",
        "### Example Usage\n",
        "Here’s how you might use some of these parameters in a practical example:\n",
        "\n",
        "```python\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train model with specific parameters\n",
        "decision_tree = DecisionTreeClassifier(\n",
        "    criterion='entropy',\n",
        "    splitter='best',\n",
        "    max_depth=5,\n",
        "    min_samples_split=4,\n",
        "    min_samples_leaf=2,\n",
        "    max_features='sqrt',\n",
        "    random_state=42\n",
        ")\n",
        "decision_tree.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate model\n",
        "y_pred = decision_tree.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Decision Tree Accuracy:\", accuracy)\n",
        "```\n",
        "\n",
        "This example customizes the `DecisionTreeClassifier` by specifying various parameters to control the behavior of the decision tree."
      ],
      "metadata": {
        "id": "YtEexvAahJ20"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train model\n",
        "decision_tree = DecisionTreeClassifier(criterion='gini', max_depth=None, random_state=42)\n",
        "decision_tree.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate model\n",
        "y_pred = decision_tree.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Decision Tree Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wvVRc9eYbz5h",
        "outputId": "4584f6af-1f94-49c1-d245-0673f767f539"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### RANDOM FOREST"
      ],
      "metadata": {
        "id": "f6naIuvpe2hx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Parameters\n",
        "The `RandomForestClassifier` in scikit-learn is a powerful ensemble method for classification tasks. Let's discuss each of its parameters:\n",
        "\n",
        "\n",
        "1. **n_estimators**:\n",
        "   - Type: `int`, default=`100`\n",
        "   - Description: The number of trees in the forest. More trees generally lead to better performance but increase the computational cost.\n",
        "\n",
        "2. **criterion**:\n",
        "   - Type: `{\"gini\", \"entropy\"}`, default=`\"gini\"`\n",
        "   - Description: The function used to measure the quality of a split. Supported criteria are:\n",
        "     - `\"gini\"`: Gini impurity.\n",
        "     - `\"entropy\"`: Information gain.\n",
        "\n",
        "3. **max_depth**:\n",
        "   - Type: `int`, default=`None`\n",
        "   - Description: The maximum depth of the tree. If `None`, then nodes are expanded until all leaves are pure or until all leaves contain less than `min_samples_split` samples.\n",
        "\n",
        "4. **min_samples_split**:\n",
        "   - Type: `int` or `float`, default=`2`\n",
        "   - Description: The minimum number of samples required to split an internal node.\n",
        "     - If `int`, then consider `min_samples_split` as the minimum number.\n",
        "     - If `float`, then `min_samples_split` is a fraction and `ceil(min_samples_split * n_samples)` are the minimum number of samples for each split.\n",
        "\n",
        "5. **min_samples_leaf**:\n",
        "   - Type: `int` or `float`, default=`1`\n",
        "   - Description: The minimum number of samples required to be at a leaf node.\n",
        "\n",
        "6. **min_weight_fraction_leaf**:\n",
        "   - Type: `float`, default=`0.0`\n",
        "   - Description: The minimum weighted fraction of the sum total of weights (of all the input samples) required to be at a leaf node.\n",
        "\n",
        "7. **max_features**:\n",
        "   - Type: `int`, `float`, `{\"auto\", \"sqrt\", \"log2\"}`, default=`\"auto\"`\n",
        "   - Description: The number of features to consider when looking for the best split.\n",
        "     - If `int`, then consider `max_features` features at each split.\n",
        "     - If `float`, then `max_features` is a fraction and `int(max_features * n_features)` features are considered at each split.\n",
        "     - If `\"auto\"`, then `max_features=sqrt(n_features)`.\n",
        "     - If `\"sqrt\"`, then `max_features=sqrt(n_features)` (same as `\"auto\"`).\n",
        "     - If `\"log2\"`, then `max_features=log2(n_features)`.\n",
        "\n",
        "8. **max_leaf_nodes**:\n",
        "   - Type: `int`, default=`None`\n",
        "   - Description: Grow a tree with `max_leaf_nodes` in best-first fashion.\n",
        "\n",
        "9. **min_impurity_decrease**:\n",
        "   - Type: `float`, default=`0.0`\n",
        "   - Description: A node will be split if this split induces a decrease of the impurity greater than or equal to this value.\n",
        "\n",
        "10. **bootstrap**:\n",
        "    - Type: `bool`, default=`True`\n",
        "    - Description: Whether bootstrap samples are used when building trees. If `False`, the whole dataset is used to build each tree.\n",
        "\n",
        "11. **oob_score**:\n",
        "    - Type: `bool`, default=`False`\n",
        "    - Description: Whether to use out-of-bag samples to estimate the generalization accuracy.\n",
        "\n",
        "12. **n_jobs**:\n",
        "    - Type: `int` or `None`, default=`None`\n",
        "    - Description: The number of jobs to run in parallel for both `fit` and `predict`. `None` means 1 unless in a `joblib.parallel_backend` context. `-1` means using all processors.\n",
        "\n",
        "13. **random_state**:\n",
        "    - Type: `int`, `RandomState` instance, or `None`, default=`None`\n",
        "    - Description: Controls the randomness of the estimator.\n",
        "\n",
        "14. **verbose**:\n",
        "    - Type: `int`, default=`0`\n",
        "    - Description: Controls the verbosity when fitting and predicting.\n",
        "\n",
        "15. **warm_start**:\n",
        "    - Type: `bool`, default=`False`\n",
        "    - Description: When set to `True`, reuse the solution of the previous call to `fit` and add more estimators to the ensemble, otherwise, just fit a whole new forest.\n",
        "\n",
        "16. **class_weight**:\n",
        "    - Type: `dict`, `list of dict`, `balanced`, or `None`, default=`None`\n",
        "    - Description: Weights associated with classes in the form `{class_label: weight}`. If `None`, all classes are supposed to have weight one. The `balanced` mode uses the values of `y` to automatically adjust weights inversely proportional to class frequencies in the input data.\n",
        "\n",
        "### Example Usage\n",
        "Here’s an example of how you might use some of these parameters in a practical scenario:\n",
        "\n",
        "```python\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train model with specific parameters\n",
        "random_forest = RandomForestClassifier(\n",
        "    n_estimators=100,\n",
        "    criterion='entropy',\n",
        "    max_depth=5,\n",
        "    min_samples_split=4,\n",
        "    min_samples_leaf=2,\n",
        "    max_features='sqrt',\n",
        "    random_state=42\n",
        ")\n",
        "random_forest.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate model\n",
        "y_pred = random_forest.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Random Forest Accuracy:\", accuracy)\n",
        "```\n",
        "\n",
        "This example customizes the `RandomForestClassifier` by specifying various parameters to control the behavior of the random forest."
      ],
      "metadata": {
        "id": "bhsenwoYhp9i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Load dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train model\n",
        "random_forest = RandomForestClassifier(n_estimators=100, criterion='gini', max_depth=None, random_state=42)\n",
        "random_forest.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate model\n",
        "y_pred = random_forest.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Random Forest Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BLNtiAxje0Mx",
        "outputId": "d121ccc1-5dd1-46ec-c930-89c19a712d66"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### CART (CLASSIFICATION AND REGRESSION TREE)"
      ],
      "metadata": {
        "id": "79NucROpe7N5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# Load dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train model\n",
        "cart = DecisionTreeClassifier(criterion='gini', max_depth=None, random_state=42)\n",
        "cart.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate model\n",
        "y_pred = cart.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"CART Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N9JCk_I2e42B",
        "outputId": "1f5c78dc-ee68-4f67-d856-fdc582905256"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CART Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ID3 (ITERATIVE DICHOTOMISER 3)"
      ],
      "metadata": {
        "id": "3T5SiBDAfEW6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Install the library: pip install decision-tree-id3\n",
        "# # !pip install decision-tree-id3\n",
        "# from id3 import Id3Estimator\n",
        "# from sklearn.datasets import load_iris\n",
        "# from sklearn.model_selection import train_test_split\n",
        "# from sklearn.metrics import accuracy_score\n",
        "\n",
        "# # Load dataset\n",
        "# iris = load_iris()\n",
        "# X, y = iris.data, iris.target\n",
        "\n",
        "# # Split dataset\n",
        "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# # Train model\n",
        "# id3 = Id3Estimator()\n",
        "# id3.fit(X_train, y_train)\n",
        "\n",
        "# # Evaluate model\n",
        "# y_pred = id3.predict(X_test)\n",
        "# accuracy = accuracy_score(y_test, y_pred)\n",
        "# print(\"ID3 Accuracy:\", accuracy)\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from collections import Counter\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "df = pd.DataFrame(X, columns=iris.feature_names)\n",
        "df['target'] = y\n",
        "\n",
        "# Split dataset\n",
        "train, test = train_test_split(df, test_size=0.2, random_state=42)\n",
        "\n",
        "# Helper function to calculate entropy\n",
        "def entropy(s):\n",
        "    counts = np.bincount(s)\n",
        "    probabilities = counts[np.nonzero(counts)] / len(s)\n",
        "    return -np.sum(probabilities * np.log2(probabilities))\n",
        "\n",
        "# Helper function to calculate information gain\n",
        "def information_gain(df, feature, target_name='target'):\n",
        "    total_entropy = entropy(df[target_name])\n",
        "    values, counts = np.unique(df[feature], return_counts=True)\n",
        "    weighted_entropy = sum((counts[i] / np.sum(counts)) * entropy(df.where(df[feature] == values[i]).dropna()[target_name]) for i in range(len(values)))\n",
        "    return total_entropy - weighted_entropy\n",
        "\n",
        "# Function to create the ID3 tree\n",
        "def id3(df, target_name='target', features=None, parent_node_class=None):\n",
        "    if features is None:\n",
        "        features = df.columns.drop(target_name)\n",
        "    # If all target values have the same value, return this value\n",
        "    if len(np.unique(df[target_name])) <= 1:\n",
        "        return np.unique(df[target_name])[0]\n",
        "    # If dataset is empty or feature set is empty, return parent node class\n",
        "    elif len(df) == 0 or len(features) == 0:\n",
        "        return parent_node_class\n",
        "    else:\n",
        "        parent_node_class = np.argmax(np.bincount(df[target_name]))\n",
        "        item_values = [information_gain(df, feature, target_name) for feature in features]\n",
        "        best_feature_index = np.argmax(item_values)\n",
        "        best_feature = features[best_feature_index]\n",
        "        tree = {best_feature: {}}\n",
        "        features = [i for i in features if i != best_feature]\n",
        "        for value in np.unique(df[best_feature]):\n",
        "            sub_data = df.where(df[best_feature] == value).dropna()\n",
        "            subtree = id3(sub_data, target_name, features, parent_node_class)\n",
        "            tree[best_feature][value] = subtree\n",
        "        return tree\n",
        "\n",
        "# Function to predict using the ID3 tree\n",
        "def predict(tree, instance):\n",
        "    if not isinstance(tree, dict):\n",
        "        return tree\n",
        "    feature = next(iter(tree))\n",
        "    value = instance[feature]\n",
        "    if value in tree[feature]:\n",
        "        return predict(tree[feature][value], instance)\n",
        "    else:\n",
        "        return 0\n",
        "\n",
        "# Train the ID3 model\n",
        "id3_tree = id3(train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = test.apply(lambda x: predict(id3_tree, x), axis=1)\n",
        "accuracy = accuracy_score(test['target'], y_pred)\n",
        "print(\"ID3 Accuracy:\", accuracy)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aYmfBQDvfBYR",
        "outputId": "0e50128c-8df8-4c3a-8d26-5f3e4f28b685"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ID3 Accuracy: 0.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### CHAID (CHI-SQUARE AUTOMATIC INTERACTION DETECTION)"
      ],
      "metadata": {
        "id": "kowSxBPRfRMe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Parameters\n",
        "The `KBinsDiscretizer` transformer in scikit-learn is used for discretization of continuous features. Let's discuss each of its parameters:\n",
        "\n",
        "\n",
        "1. **n_bins**:\n",
        "   - Type: `int` or array-like of shape (n_features,), default=`5`\n",
        "   - Description: The number of bins to discretize each feature into. If a single int is provided, the same number of bins is used for all features. If an array-like is provided, it must have the same length as the number of features, and each element specifies the number of bins for the corresponding feature.\n",
        "\n",
        "2. **encode**:\n",
        "   - Type: `{'onehot', 'onehot-dense', 'ordinal'}`, default=`'onehot'`\n",
        "   - Description: The method used to encode the transformed result:\n",
        "     - `'onehot'`: Encode the transformed result using a one-hot (one-of-K) encoding scheme. Outputs will be sparse if the input is sparse and dense otherwise.\n",
        "     - `'onehot-dense'`: Encode the transformed result using a one-hot (one-of-K) encoding scheme. Outputs will always be dense.\n",
        "     - `'ordinal'`: Encode the transformed result as an integer array. The categories are encoded as consecutive integers.\n",
        "\n",
        "3. **strategy**:\n",
        "   - Type: `{'uniform', 'quantile', 'kmeans'}`, default=`'quantile'`\n",
        "   - Description: The strategy used to define the widths of the bins:\n",
        "     - `'uniform'`: All bins in each feature have identical widths.\n",
        "     - `'quantile'`: All bins in each feature have the same number of points.\n",
        "     - `'kmeans'`: Values in each bin have approximately the same number of points.\n",
        "\n",
        "4. **(Deprecated) strategy_kwargs**:\n",
        "   - Type: `dict`, default=`None`\n",
        "   - Description: Additional parameters passed to the strategy function. This parameter is deprecated and will be removed in future versions of scikit-learn.\n",
        "\n",
        "5. **(Deprecated) encode_unknown**:\n",
        "   - Type: `{'error', 'missing', 'ignore'}`, default=`'error'`\n",
        "   - Description: Determines behavior for unknown/NaN values during transform. This parameter is deprecated and will be removed in future versions of scikit-learn.\n",
        "\n",
        "### Example Usage\n",
        "Here's how you might use the `KBinsDiscretizer` transformer with some specific parameters:\n",
        "\n",
        "```python\n",
        "from sklearn.preprocessing import KBinsDiscretizer\n",
        "import numpy as np\n",
        "\n",
        "# Create some continuous data\n",
        "X = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
        "\n",
        "# Initialize the KBinsDiscretizer with specific parameters\n",
        "discretizer = KBinsDiscretizer(n_bins=3, encode='ordinal', strategy='uniform')\n",
        "\n",
        "# Fit and transform the data\n",
        "X_discretized = discretizer.fit_transform(X)\n",
        "\n",
        "# Display the transformed data\n",
        "print(X_discretized)\n",
        "```\n",
        "\n",
        "In this example, the `KBinsDiscretizer` is initialized with `n_bins=3` to create three bins, `encode='ordinal'` to encode the transformed result as an integer array, and `strategy='uniform'` to make all bins in each feature have identical widths. The continuous data `X` is then transformed into discrete bins according to these parameters."
      ],
      "metadata": {
        "id": "CBorQ-PEiVVP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install the library: pip install CHAID\n",
        "from CHAID import Tree\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import KBinsDiscretizer\n",
        "\n",
        "# Load dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Discretize continuous features into categorical features\n",
        "discretizer = KBinsDiscretizer(n_bins=3, encode='ordinal', strategy='uniform')\n",
        "X_discretized = discretizer.fit_transform(X)\n",
        "\n",
        "# Create DataFrame\n",
        "df = pd.DataFrame(X_discretized, columns=iris.feature_names)\n",
        "df['target'] = y\n",
        "\n",
        "# Split dataset\n",
        "train, test = train_test_split(df, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the variable types (nominal)\n",
        "variables = {col: 'nominal' for col in df.columns}\n",
        "\n",
        "# Train model\n",
        "chaid_tree = Tree.from_pandas_df(train, variables, 'target')\n",
        "chaid_tree.print_tree()\n",
        "\n",
        "# CHAID library does not directly support predictions and accuracy evaluations\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ec-TyyZrfN8W",
        "outputId": "96258403-52c0-417e-a9f1-6a2fab9a41df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "b'([], {0: 40.0, 1: 41.0, 2: 39.0}, (target, p=9.2778541692036e-51, score=240.0, groups=[[0.0], [1.0], [2.0]]), dof=4))\\n|-- ([0.0], {0: 40.0, 1: 0, 2: 0}, <Invalid Chaid Split> - the node only contains single category respondents)\\n|-- ([1.0], {0: 0, 1: 41.0, 2: 0}, <Invalid Chaid Split> - the node only contains single category respondents)\\n+-- ([2.0], {0: 0, 1: 0, 2: 39.0}, <Invalid Chaid Split> - the node only contains single category respondents)\\n'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BAYESIAN NETWORK"
      ],
      "metadata": {
        "id": "_uDI8s2gilCu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### GUASSIAN , MULTINOMIAL AND BERNOLLI NAIVE BAYES"
      ],
      "metadata": {
        "id": "pG5BE7tli_s0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Gaussian Naive Bayes\n",
        "gnb = GaussianNB()\n",
        "gnb.fit(X_train, y_train)\n",
        "gnb_pred = gnb.predict(X_test)\n",
        "gnb_accuracy = accuracy_score(y_test, gnb_pred)\n",
        "print(\"Gaussian Naive Bayes Accuracy:\", gnb_accuracy)\n",
        "\n",
        "# Multinomial Naive Bayes\n",
        "mnb = MultinomialNB()\n",
        "mnb.fit(X_train, y_train)\n",
        "mnb_pred = mnb.predict(X_test)\n",
        "mnb_accuracy = accuracy_score(y_test, mnb_pred)\n",
        "print(\"Multinomial Naive Bayes Accuracy:\", mnb_accuracy)\n",
        "\n",
        "# Bernoulli Naive Bayes\n",
        "# Note: BernoulliNB is not suitable for iris dataset because it expects binary data\n",
        "# Here is just an example of how to use it with another dataset\n",
        "bnb = BernoulliNB()\n",
        "bnb.fit(X_train, y_train)\n",
        "bnb_pred = bnb.predict(X_test)\n",
        "bnb_accuracy = accuracy_score(y_test, bnb_pred)\n",
        "print(\"Bernoulli Naive Bayes Accuracy:\", bnb_accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f8tXaY51fYfT",
        "outputId": "e9f1c2a9-b606-4f60-be83-ad499ce5e95f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gaussian Naive Bayes Accuracy: 1.0\n",
            "Multinomial Naive Bayes Accuracy: 0.9\n",
            "Bernoulli Naive Bayes Accuracy: 0.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AK1rodtNjVBx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}